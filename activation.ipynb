{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MHYqTVwBfcwQ"
      },
      "outputs": [],
      "source": [
        "#SFCNN WITHOUT EXPANDING\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GSiLU(nn.Module):\n",
        "    \"\"\"Global Sigmoid Linear Unit proposed in the paper\n",
        "    Returns x * sigmoid(global average pooling of x)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(GSiLU, self).__init__()\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gap = self.gap(x)\n",
        "        return x * torch.sigmoid(gap)\n",
        "\n",
        "\n",
        "class DWCONV(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(DWCONV, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=in_channels, bias=False)\n",
        "\n",
        "\n",
        "activation = nn.SiLU()\n",
        "class SFCNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=False):\n",
        "        super(SFCNNBlock, self).__init__()\n",
        "\n",
        "        self.downsample = downsample #used in Type 2 with downsample\n",
        "        if downsample : stride=2\n",
        "\n",
        "        #1 Applying 3x3 DWConv (groups parameter separates channels, performing depthwise convolution)\n",
        "        self.dwconv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False)\n",
        "        #2 Pass through PWConv and SiLU\n",
        "        self.pwconv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.silu = nn.SiLU()\n",
        "        #3 Applying 3x3 DWConv\n",
        "        self.dwconv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, groups=out_channels, bias=False)\n",
        "        #4 Pass through GSILU\n",
        "        self.gsilu = activation\n",
        "        #5 Pass through PWConv\n",
        "        self.pwconv2 = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "\n",
        "        #7 Passing x through these at the end\n",
        "        if self.downsample:\n",
        "            self.layernorm = nn.LayerNorm(in_channels)\n",
        "            self.downsample_conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "            self.downsample_pw = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        out = x\n",
        "\n",
        "        if self.downsample:\n",
        "            B, C, H, W = out.size()\n",
        "            out = out.permute(0, 2, 3, 1)  # B, H, W, C (need to put C in last pos for LN)\n",
        "            out = self.layernorm(out) # apply layernorm in the beginning\n",
        "            # print(\"LayerNorm finished\")\n",
        "            out = out.permute(0, 3, 1, 2) # back to original shape\n",
        "\n",
        "            #7 If downsample is True, apply 3x3 DWConv and PWConv to input x\n",
        "            input_downsampled = self.downsample_conv(x)\n",
        "            input_downsampled = self.downsample_pw(input_downsampled)\n",
        "\n",
        "        out = self.dwconv1(input)\n",
        "        # print(\"DWConv1 finished\")\n",
        "        out = self.pwconv1(out)\n",
        "        # print(\"PWConv1 finished\")\n",
        "        out = self.silu(out)\n",
        "\n",
        "        out = self.dwconv2(out)\n",
        "        # print(\"DWConv2 finished\")\n",
        "        out = self.gsilu(out)\n",
        "        out = self.pwconv2(out) # Output of step 5\n",
        "        # print(\"PWConv2 finished\")\n",
        "\n",
        "\n",
        "        #6 Input of step 1 and output of step 5 are added\n",
        "        # print(\"Input shape: \", input.shape)\n",
        "        # print(\"Output shape: \", out.shape)\n",
        "        out += input_downsampled if self.downsample else input\n",
        "        return out\n",
        "\n",
        "\n",
        "class SFCNN(nn.Module):\n",
        "    def __init__(self, num_classes=1000, block_numbers=[4, 8, 20, 4], channels=[48, 96, 192, 384]):\n",
        "        super(SFCNN, self).__init__()\n",
        "\n",
        "        self.stem = nn.Conv2d(3, channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n",
        "\n",
        "        self.stage1 = self.make_stage(block_numbers[0], channels[0], channels[1], stride=2)\n",
        "        self.stage2 = self.make_stage(block_numbers[1], channels[1], channels[2], stride=2)\n",
        "        self.stage3 = self.make_stage(block_numbers[2], channels[2], channels[3], stride=2)\n",
        "        self.stage4 = self.make_stage(block_numbers[3], channels[3], channels[3], stride=1)\n",
        "\n",
        "        self.last_conv = nn.Conv2d(channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, num_classes)\n",
        "        )\n",
        "\n",
        "    def make_stage(self, block_nb, in_channels, out_channels, stride):\n",
        "        layers = []\n",
        "        layers.append(SFCNNBlock(in_channels, out_channels, stride=stride, downsample=True))\n",
        "        for _ in range(1, block_nb):\n",
        "            layers.append(SFCNNBlock(out_channels, out_channels, stride=1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        # print(\"Stem finished\")\n",
        "        x = self.stage1(x)\n",
        "        # print(\"Stage 1 finished\")\n",
        "        x = self.stage2(x)\n",
        "        # print(\"Stage 2 finished\")\n",
        "        x = self.stage3(x)\n",
        "        # print(\"Stage 3 finished\")\n",
        "        x = self.stage4(x)\n",
        "        # print(\"Stage 4 finished\")\n",
        "\n",
        "        x = self.last_conv(x)\n",
        "        # print(\"Last Conv finished\")\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48LaiQP0lh0K",
        "outputId": "1762ddbf-bdcf-498c-bbe2-0ff2fca78586"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "# Hyperparam√®tres\n",
        "batch_size = 128\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.05\n",
        "warmup_epochs = 5\n",
        "\n",
        "# Preprocessing and augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616]),  # CIFAR stats\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616]),\n",
        "])\n",
        "\n",
        "#loading the dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "val_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Init the model, loss function, and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_classes = 10 #100 for cifar100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "675Nbamnlgqm",
        "outputId": "012a218a-4cd0-4156-e813-e497f19a68ca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170M/170M [00:03<00:00, 48.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter(\"/content/drive/MyDrive/runs/sfcnn_pico_cifar10_gsilu\")\n",
        "\n",
        "print(\"Creating SFCNN-P_silu\")\n",
        "channels = [32*2**i for i in range(4)]\n",
        "model = SFCNN(num_classes=num_classes, block_numbers=[3, 4, 12, 3], channels=channels).to(device)\n",
        "print(\"Executing SFCNN-P\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs - warmup_epochs)\n",
        "\n",
        "# Warmup scheduler\n",
        "def warmup_scheduler(epoch, warmup_epochs, optimizer):\n",
        "    if epoch < warmup_epochs:\n",
        "        lr = learning_rate * (epoch + 1) / warmup_epochs\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    warmup_scheduler(epoch, warmup_epochs, optimizer)\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        #print(\"starting forward pass\")\n",
        "        outputs = model(inputs)\n",
        "        #print(\"outputs calculated\")\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # if i % 100 == 0:\n",
        "        #     print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100. * correct / total\n",
        "    writer.add_scalar('Loss/Train', train_loss, epoch)\n",
        "    writer.add_scalar('Accuracy/Train', train_acc, epoch)\n",
        "    writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)\n",
        "\n",
        "    if epoch >= warmup_epochs:\n",
        "        scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = 100. * val_correct / val_total\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, Validation Acc: {val_acc:.2f}%')\n",
        "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
        "    writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
        "\n",
        "# Save the final model\n",
        "#torch.save(model.state_dict(), 'models/sfcnn_tiny_cifar10_run2.pth')\n",
        "writer.close()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM2AplgZhx78",
        "outputId": "b0d56070-fdb5-46eb-b448-3b765d1a973c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating SFCNN-P_silu\n",
            "Executing SFCNN-P\n",
            "Epoch [1/100], Validation Loss: 1.8257, Validation Acc: 34.65%\n",
            "Epoch [2/100], Validation Loss: 1.7477, Validation Acc: 37.57%\n",
            "Epoch [3/100], Validation Loss: 1.7148, Validation Acc: 38.60%\n",
            "Epoch [4/100], Validation Loss: 1.6471, Validation Acc: 40.91%\n",
            "Epoch [5/100], Validation Loss: 1.6470, Validation Acc: 39.59%\n",
            "Epoch [6/100], Validation Loss: 1.5731, Validation Acc: 43.45%\n",
            "Epoch [7/100], Validation Loss: 1.5244, Validation Acc: 44.45%\n",
            "Epoch [8/100], Validation Loss: 1.5253, Validation Acc: 44.98%\n",
            "Epoch [9/100], Validation Loss: 1.4653, Validation Acc: 46.25%\n",
            "Epoch [10/100], Validation Loss: 1.4638, Validation Acc: 46.64%\n",
            "Epoch [11/100], Validation Loss: 1.4253, Validation Acc: 47.31%\n",
            "Epoch [12/100], Validation Loss: 1.4198, Validation Acc: 48.48%\n",
            "Epoch [13/100], Validation Loss: 1.3585, Validation Acc: 50.20%\n",
            "Epoch [14/100], Validation Loss: 1.3475, Validation Acc: 51.17%\n",
            "Epoch [15/100], Validation Loss: 1.3257, Validation Acc: 51.77%\n",
            "Epoch [16/100], Validation Loss: 1.3277, Validation Acc: 51.61%\n",
            "Epoch [17/100], Validation Loss: 1.3249, Validation Acc: 51.68%\n",
            "Epoch [18/100], Validation Loss: 1.2528, Validation Acc: 54.38%\n",
            "Epoch [19/100], Validation Loss: 1.2701, Validation Acc: 54.14%\n",
            "Epoch [20/100], Validation Loss: 1.2656, Validation Acc: 54.42%\n",
            "Epoch [21/100], Validation Loss: 1.2283, Validation Acc: 55.14%\n",
            "Epoch [22/100], Validation Loss: 1.1867, Validation Acc: 57.31%\n",
            "Epoch [23/100], Validation Loss: 1.1899, Validation Acc: 57.21%\n",
            "Epoch [24/100], Validation Loss: 1.1873, Validation Acc: 57.38%\n",
            "Epoch [25/100], Validation Loss: 1.1566, Validation Acc: 58.79%\n",
            "Epoch [26/100], Validation Loss: 1.1330, Validation Acc: 59.52%\n",
            "Epoch [27/100], Validation Loss: 1.1391, Validation Acc: 59.58%\n",
            "Epoch [28/100], Validation Loss: 1.0920, Validation Acc: 61.29%\n",
            "Epoch [29/100], Validation Loss: 1.0686, Validation Acc: 62.14%\n",
            "Epoch [30/100], Validation Loss: 1.0771, Validation Acc: 62.01%\n",
            "Epoch [31/100], Validation Loss: 1.0490, Validation Acc: 62.74%\n",
            "Epoch [32/100], Validation Loss: 1.0609, Validation Acc: 62.18%\n",
            "Epoch [33/100], Validation Loss: 1.0474, Validation Acc: 63.35%\n",
            "Epoch [34/100], Validation Loss: 1.0360, Validation Acc: 63.44%\n",
            "Epoch [35/100], Validation Loss: 0.9815, Validation Acc: 65.00%\n",
            "Epoch [36/100], Validation Loss: 0.9664, Validation Acc: 65.93%\n",
            "Epoch [37/100], Validation Loss: 1.0010, Validation Acc: 64.55%\n",
            "Epoch [38/100], Validation Loss: 0.9748, Validation Acc: 65.82%\n",
            "Epoch [39/100], Validation Loss: 0.9365, Validation Acc: 67.03%\n",
            "Epoch [40/100], Validation Loss: 0.9419, Validation Acc: 66.31%\n",
            "Epoch [41/100], Validation Loss: 0.9361, Validation Acc: 67.04%\n",
            "Epoch [42/100], Validation Loss: 0.9564, Validation Acc: 66.50%\n",
            "Epoch [43/100], Validation Loss: 0.9338, Validation Acc: 67.40%\n",
            "Epoch [44/100], Validation Loss: 0.9251, Validation Acc: 67.61%\n",
            "Epoch [45/100], Validation Loss: 0.9249, Validation Acc: 68.44%\n",
            "Epoch [46/100], Validation Loss: 0.9077, Validation Acc: 68.46%\n",
            "Epoch [47/100], Validation Loss: 0.9106, Validation Acc: 68.35%\n",
            "Epoch [48/100], Validation Loss: 0.9177, Validation Acc: 68.18%\n",
            "Epoch [49/100], Validation Loss: 0.9195, Validation Acc: 68.21%\n",
            "Epoch [50/100], Validation Loss: 0.9262, Validation Acc: 68.01%\n",
            "Epoch [51/100], Validation Loss: 0.9056, Validation Acc: 68.53%\n",
            "Epoch [52/100], Validation Loss: 0.8960, Validation Acc: 69.46%\n",
            "Epoch [53/100], Validation Loss: 0.9217, Validation Acc: 68.69%\n",
            "Epoch [54/100], Validation Loss: 0.9278, Validation Acc: 68.48%\n",
            "Epoch [55/100], Validation Loss: 0.9166, Validation Acc: 69.33%\n",
            "Epoch [56/100], Validation Loss: 0.9115, Validation Acc: 69.04%\n",
            "Epoch [57/100], Validation Loss: 0.9094, Validation Acc: 69.50%\n",
            "Epoch [58/100], Validation Loss: 0.9139, Validation Acc: 69.16%\n",
            "Epoch [59/100], Validation Loss: 0.9152, Validation Acc: 69.60%\n",
            "Epoch [60/100], Validation Loss: 0.9126, Validation Acc: 69.87%\n",
            "Epoch [61/100], Validation Loss: 0.9324, Validation Acc: 69.16%\n",
            "Epoch [62/100], Validation Loss: 0.9296, Validation Acc: 69.87%\n",
            "Epoch [63/100], Validation Loss: 0.9766, Validation Acc: 68.37%\n",
            "Epoch [64/100], Validation Loss: 0.9784, Validation Acc: 69.03%\n",
            "Epoch [65/100], Validation Loss: 0.9602, Validation Acc: 69.69%\n",
            "Epoch [66/100], Validation Loss: 1.0040, Validation Acc: 68.99%\n",
            "Epoch [67/100], Validation Loss: 0.9954, Validation Acc: 69.38%\n",
            "Epoch [68/100], Validation Loss: 0.9820, Validation Acc: 70.09%\n",
            "Epoch [69/100], Validation Loss: 0.9959, Validation Acc: 69.57%\n",
            "Epoch [70/100], Validation Loss: 1.0241, Validation Acc: 69.77%\n",
            "Epoch [71/100], Validation Loss: 1.0427, Validation Acc: 69.50%\n",
            "Epoch [72/100], Validation Loss: 1.0536, Validation Acc: 69.83%\n",
            "Epoch [73/100], Validation Loss: 1.0636, Validation Acc: 69.14%\n",
            "Epoch [74/100], Validation Loss: 1.0823, Validation Acc: 69.38%\n",
            "Epoch [75/100], Validation Loss: 1.0777, Validation Acc: 69.70%\n",
            "Epoch [76/100], Validation Loss: 1.1188, Validation Acc: 69.43%\n",
            "Epoch [77/100], Validation Loss: 1.1155, Validation Acc: 69.55%\n",
            "Epoch [78/100], Validation Loss: 1.1580, Validation Acc: 69.61%\n",
            "Epoch [79/100], Validation Loss: 1.1613, Validation Acc: 69.58%\n",
            "Epoch [80/100], Validation Loss: 1.1761, Validation Acc: 69.05%\n",
            "Epoch [81/100], Validation Loss: 1.2004, Validation Acc: 69.10%\n",
            "Epoch [82/100], Validation Loss: 1.2117, Validation Acc: 68.97%\n",
            "Epoch [83/100], Validation Loss: 1.2155, Validation Acc: 69.17%\n",
            "Epoch [84/100], Validation Loss: 1.2264, Validation Acc: 69.60%\n",
            "Epoch [85/100], Validation Loss: 1.2606, Validation Acc: 69.32%\n",
            "Epoch [86/100], Validation Loss: 1.2476, Validation Acc: 68.99%\n",
            "Epoch [87/100], Validation Loss: 1.2563, Validation Acc: 69.18%\n",
            "Epoch [88/100], Validation Loss: 1.3011, Validation Acc: 68.82%\n",
            "Epoch [89/100], Validation Loss: 1.2950, Validation Acc: 69.24%\n",
            "Epoch [90/100], Validation Loss: 1.2906, Validation Acc: 69.41%\n",
            "Epoch [91/100], Validation Loss: 1.3025, Validation Acc: 69.39%\n",
            "Epoch [92/100], Validation Loss: 1.3088, Validation Acc: 69.22%\n",
            "Epoch [93/100], Validation Loss: 1.3229, Validation Acc: 69.35%\n",
            "Epoch [94/100], Validation Loss: 1.3240, Validation Acc: 69.17%\n",
            "Epoch [95/100], Validation Loss: 1.3332, Validation Acc: 69.22%\n",
            "Epoch [96/100], Validation Loss: 1.3301, Validation Acc: 69.06%\n",
            "Epoch [97/100], Validation Loss: 1.3319, Validation Acc: 69.01%\n",
            "Epoch [98/100], Validation Loss: 1.3326, Validation Acc: 69.12%\n",
            "Epoch [99/100], Validation Loss: 1.3321, Validation Acc: 69.14%\n",
            "Epoch [100/100], Validation Loss: 1.3324, Validation Acc: 69.14%\n"
          ]
        }
      ]
    }
  ]
}